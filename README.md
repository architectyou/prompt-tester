# ğŸ¯ Prompt Tester

A local LLM prompt testing tool inspired by LangSmith Prompter Test.
<br>
Built with Streamlit, this tool allows you to compare two different prompt versions or test a single prompt multiple times.

## âœ¨ Features

- ğŸ“ Support for single/double prompt testing
- ğŸ”„ Separate system and human prompt inputs
- ğŸ” Multiple test iterations (up to 10)
- ğŸŒ¡ï¸ Adjustable temperature (0.0-1.0)
- â±ï¸ Inference time measurement
- ğŸ’¾ Prompt save and reset functionality
- ğŸˆ You can know about the inference time and the result of the prompt with model's thinking process.

## ğŸ® Demo

This tool works with any OpenAI API-compatible LLM server (e.g., sglang, VLLM).

### ğŸ” Test Single Prompt

![test1](readme_imgs/image1.png)

- Test with a single prompt set
- Input system and human prompts
- Run multiple iterations

### ğŸ”„ Test Double Prompt

![test2](readme_imgs/image2.png)

- Compare two different prompt sets side by side
- View results simultaneously
- Compare inference times

## ğŸš€ Installation & Usage

1. Install requirements

```bash
pip install -r requirements.txt
```

2. Configure environment

- Create `.env` file with:
  ```
  BASE_URL=your_llm_server_url
  API_KEY=your_api_key
  ```

3. Run the application

```bash
streamlit run prompt_tester.py
```

4. Access the web interface

```
http://localhost:8501
```

## ğŸ“‹ Requirements

- Python 3.8+
- OpenAI API compatible LLM server
- Streamlit
- python-dotenv
- openai

## ğŸ“ Notes

- ğŸ”Œ Compatible with OpenAI API format LLM servers
- ğŸ¤– Default model: "./Qwen2.5-32B-Instruct-AWQ"
- ğŸšï¸ Default temperature: 0.56
- ğŸ“Š Max tokens: 300
